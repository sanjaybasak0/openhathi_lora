{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "28506e0e-b778-4803-84b1-4c859d2741fd",
      "metadata": {
        "id": "28506e0e-b778-4803-84b1-4c859d2741fd"
      },
      "source": [
        "# Instruction Finetuning using LoRA"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "623ba5b8-af16-438d-b3f3-709b65f6ac96",
      "metadata": {
        "id": "623ba5b8-af16-438d-b3f3-709b65f6ac96"
      },
      "source": [
        "In this notebook, we will look into how to perform instruction finetuning using LoRA PEFT method. The task is to perform Supervised finetuning (SFT) of OpenHathi model on a small Hingligh instruct dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b098e055-a939-4da7-879e-85849982cdcb",
      "metadata": {
        "id": "b098e055-a939-4da7-879e-85849982cdcb"
      },
      "source": [
        "Load the required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75d154b8-9996-40fa-ad02-f8f6c26e9567",
      "metadata": {
        "id": "75d154b8-9996-40fa-ad02-f8f6c26e9567",
        "outputId": "5cf67b3a-750f-4c2b-e3ea-5aeb8e145364"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/raid/sourab/transformers/src/transformers/utils/hub.py:122: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
            "  warnings.warn(\n",
            "2024-01-02 09:12:33.638699: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-02 09:12:33.638745: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-02 09:12:33.639620: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-02 09:12:33.645583: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-01-02 09:12:34.458579: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2024-01-02 09:12:36,106] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.environ[\"WANDB_PROJECT\"]=\"openhathi_instruct_finetuning\"\n",
        "\n",
        "from enum import Enum\n",
        "from functools import partial\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "from transformers import AutoModelForCausalLM, LlamaTokenizer, TrainingArguments, set_seed\n",
        "from datasets import load_dataset\n",
        "from trl import SFTTrainer\n",
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "\n",
        "seed = 42\n",
        "set_seed(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "991163c6-29f3-496e-b71c-f4329ec25df1",
      "metadata": {
        "id": "991163c6-29f3-496e-b71c-f4329ec25df1"
      },
      "source": [
        "## Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28b27033-ba17-4c85-986d-9cbef9262497",
      "metadata": {
        "id": "28b27033-ba17-4c85-986d-9cbef9262497",
        "outputId": "de5c4262-56f1-463d-ce45-a9513ec554e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['content'],\n",
            "        num_rows: 916\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['content'],\n",
            "        num_rows: 102\n",
            "    })\n",
            "})\n",
            "{'content': '<|im_start|>user\\nKya Mother Teresa Nobel Peace Prize jeeti thi?<|im_end|>\\n<|im_start|>assistant\\nHaan, Mother Teresa ne 1979 mein Nobel Peace Prize jeeti thi, unke social work aur seva ke liye.<|im_end|>\\n'}\n"
          ]
        }
      ],
      "source": [
        "model_name = \"sarvamai/OpenHathi-7B-Hi-v0.1-Base\"\n",
        "dataset_name = \"smangrul/hinglish_self_instruct_v0\"\n",
        "tokenizer = LlamaTokenizer.from_pretrained(model_name)\n",
        "template = \"\"\"{% for message in messages %}\\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% if loop.last and add_generation_prompt %}{{'<|im_start|>assistant\\n' }}{% endif %}{% endfor %}\"\"\"\n",
        "tokenizer.chat_template = template\n",
        "\n",
        "def preprocess(samples):\n",
        "    batch = []\n",
        "    for conversation in samples[\"messages\"]:\n",
        "        batch.append(tokenizer.apply_chat_template(conversation, tokenize=False))\n",
        "    return {\"content\": batch}\n",
        "\n",
        "dataset = load_dataset(dataset_name)\n",
        "dataset = dataset.map(\n",
        "    preprocess,\n",
        "    batched=True,\n",
        "    remove_columns=dataset[\"train\"].column_names\n",
        ")\n",
        "dataset = dataset[\"train\"].train_test_split(0.1)\n",
        "print(dataset)\n",
        "print(dataset[\"train\"][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c687b73b-4479-4ff4-9ed9-a0df95e9b40a",
      "metadata": {
        "id": "c687b73b-4479-4ff4-9ed9-a0df95e9b40a"
      },
      "source": [
        "## Create the PEFT model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "641dfc0c-448b-49c4-9643-ce457c0c0ed5",
      "metadata": {
        "id": "641dfc0c-448b-49c4-9643-ce457c0c0ed5"
      },
      "source": [
        "### LoRA Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91ae0542-2bc4-47da-8c69-bf122794fcac",
      "metadata": {
        "id": "91ae0542-2bc4-47da-8c69-bf122794fcac"
      },
      "outputs": [],
      "source": [
        "peft_config = LoraConfig(r=8,\n",
        "                         lora_alpha=16,\n",
        "                         lora_dropout=0.1,\n",
        "                         target_modules=[\"gate_proj\",\"q_proj\",\"lm_head\",\"o_proj\",\"k_proj\",\"embed_tokens\",\"down_proj\",\"up_proj\",\"v_proj\"],\n",
        "                         task_type=TaskType.CAUSAL_LM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ebc83f34-1c74-45af-a0a0-d2684a014647",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "f13962b178154b1cadac235a5211ff3a"
          ]
        },
        "id": "ebc83f34-1c74-45af-a0a0-d2684a014647",
        "outputId": "4f4f3178-a1d0-475c-ba07-c6de85e8362d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f13962b178154b1cadac235a5211ff3a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 20,823,120 || all params: 6,890,875,984 || trainable%: 0.3021839320334516\n"
          ]
        }
      ],
      "source": [
        "class ChatmlSpecialTokens(str, Enum):\n",
        "    user = \"<|im_start|>user\"\n",
        "    assistant = \"<|im_start|>assistant\"\n",
        "    system = \"<|im_start|>system\"\n",
        "    eos_token = \"<|im_end|>\"\n",
        "    bos_token = \"<s>\"\n",
        "    pad_token = \"<pad>\"\n",
        "\n",
        "    @classmethod\n",
        "    def list(cls):\n",
        "        return [c.value for c in cls]\n",
        "\n",
        "tokenizer = LlamaTokenizer.from_pretrained(\n",
        "        model_name,\n",
        "        pad_token=ChatmlSpecialTokens.pad_token.value,\n",
        "        bos_token=ChatmlSpecialTokens.bos_token.value,\n",
        "        eos_token=ChatmlSpecialTokens.eos_token.value,\n",
        "        additional_special_tokens=ChatmlSpecialTokens.list(),\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "tokenizer.chat_template = template\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "model = get_peft_model(model, peft_config)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# cast non-trainable params in fp16\n",
        "for p in model.parameters():\n",
        "    if not p.requires_grad:\n",
        "        p.data = p.to(torch.float16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c522d17-7bb5-4ae6-87af-28de73f1cb37",
      "metadata": {
        "id": "3c522d17-7bb5-4ae6-87af-28de73f1cb37",
        "outputId": "da709e20-63ee-4249-9972-922a8d9c5b71"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PeftModelForCausalLM(\n",
              "  (base_model): LoraModel(\n",
              "    (model): LlamaForCausalLM(\n",
              "      (model): LlamaModel(\n",
              "        (embed_tokens): lora.Embedding(\n",
              "          (base_layer): Embedding(48069, 4096)\n",
              "          (lora_dropout): ModuleDict(\n",
              "            (default): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (lora_A): ModuleDict()\n",
              "          (lora_B): ModuleDict()\n",
              "          (lora_embedding_A): ParameterDict(  (default): Parameter containing: [torch.FloatTensor of size 8x48069])\n",
              "          (lora_embedding_B): ParameterDict(  (default): Parameter containing: [torch.FloatTensor of size 4096x8])\n",
              "        )\n",
              "        (layers): ModuleList(\n",
              "          (0-31): 32 x LlamaDecoderLayer(\n",
              "            (self_attn): LlamaSdpaAttention(\n",
              "              (q_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "              )\n",
              "              (k_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "              )\n",
              "              (v_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "              )\n",
              "              (o_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "              )\n",
              "              (rotary_emb): LlamaRotaryEmbedding()\n",
              "            )\n",
              "            (mlp): LlamaMLP(\n",
              "              (gate_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=4096, out_features=11008, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=11008, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "              )\n",
              "              (up_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=4096, out_features=11008, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=11008, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "              )\n",
              "              (down_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=11008, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=11008, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "              )\n",
              "              (act_fn): SiLU()\n",
              "            )\n",
              "            (input_layernorm): LlamaRMSNorm()\n",
              "            (post_attention_layernorm): LlamaRMSNorm()\n",
              "          )\n",
              "        )\n",
              "        (norm): LlamaRMSNorm()\n",
              "      )\n",
              "      (lm_head): lora.Linear(\n",
              "        (base_layer): Linear(in_features=4096, out_features=48069, bias=False)\n",
              "        (lora_dropout): ModuleDict(\n",
              "          (default): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (lora_A): ModuleDict(\n",
              "          (default): Linear(in_features=4096, out_features=8, bias=False)\n",
              "        )\n",
              "        (lora_B): ModuleDict(\n",
              "          (default): Linear(in_features=8, out_features=48069, bias=False)\n",
              "        )\n",
              "        (lora_embedding_A): ParameterDict()\n",
              "        (lora_embedding_B): ParameterDict()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8987259-21a6-415d-abcf-7517588b60da",
      "metadata": {
        "id": "d8987259-21a6-415d-abcf-7517588b60da"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a78c590d-16cb-4edc-9aab-30b8137a2989",
      "metadata": {
        "id": "a78c590d-16cb-4edc-9aab-30b8137a2989"
      },
      "outputs": [],
      "source": [
        "output_dir = \"openhathi_instruct\"\n",
        "per_device_train_batch_size = 1\n",
        "per_device_eval_batch_size = 1\n",
        "gradient_accumulation_steps = 8\n",
        "logging_steps = 5\n",
        "learning_rate = 5e-4\n",
        "max_grad_norm = 1.0\n",
        "max_steps = 250\n",
        "num_train_epochs=10\n",
        "warmup_ratio = 0.1\n",
        "lr_scheduler_type = \"cosine\"\n",
        "max_seq_length = 2048\n",
        "\n",
        "training_arguments = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    per_device_train_batch_size=per_device_train_batch_size,\n",
        "    per_device_eval_batch_size=per_device_eval_batch_size,\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "    save_strategy=\"no\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    logging_steps=logging_steps,\n",
        "    learning_rate=learning_rate,\n",
        "    max_grad_norm=max_grad_norm,\n",
        "    weight_decay=0.1,\n",
        "    warmup_ratio=warmup_ratio,\n",
        "    lr_scheduler_type=lr_scheduler_type,\n",
        "    fp16=True,\n",
        "    report_to=[\"tensorboard\", \"wandb\"],\n",
        "    hub_private_repo=True,\n",
        "    push_to_hub=True,\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    gradient_checkpointing=True,\n",
        "    gradient_checkpointing_kwargs={\"use_reentrant\": False}\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc6ece17-0db8-41a6-98f9-3b2616b5852f",
      "metadata": {
        "id": "bc6ece17-0db8-41a6-98f9-3b2616b5852f",
        "outputId": "b120ce92-8368-4222-fdec-d891fdadddf2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
          ]
        }
      ],
      "source": [
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=training_arguments,\n",
        "    train_dataset=dataset[\"train\"],\n",
        "    eval_dataset=dataset[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    packing=True,\n",
        "    dataset_text_field=\"content\",\n",
        "    max_seq_length=max_seq_length,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "546810f1-4dc1-44b6-b2d7-f91aa7758533",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "0c6fd6a7f58b44a38ee923b49092f99a",
            "18087eef8b0d4e3ebd72e32d485571af",
            "6e30694521aa40429b97fb641f94b2d7",
            "b8e5ae82077b4b2e81250f478f488eb9"
          ]
        },
        "id": "546810f1-4dc1-44b6-b2d7-f91aa7758533",
        "outputId": "63510c52-2d4a-49e3-eaef-e4a8a59ee08e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msmangrul\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.16.1 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.16.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/raid/sourab/temp/wandb/run-20240102_091253-yf3l235s</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/smangrul/openhathi_instruct_finetuning/runs/yf3l235s' target=\"_blank\">trim-capybara-5</a></strong> to <a href='https://wandb.ai/smangrul/openhathi_instruct_finetuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/smangrul/openhathi_instruct_finetuning' target=\"_blank\">https://wandb.ai/smangrul/openhathi_instruct_finetuning</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/smangrul/openhathi_instruct_finetuning/runs/yf3l235s' target=\"_blank\">https://wandb.ai/smangrul/openhathi_instruct_finetuning/runs/yf3l235s</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='40' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [40/40 04:51, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.557752</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.833800</td>\n",
              "      <td>1.958092</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.084000</td>\n",
              "      <td>1.798027</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.764700</td>\n",
              "      <td>1.536267</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.476700</td>\n",
              "      <td>1.433572</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.476700</td>\n",
              "      <td>1.341489</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>1.260600</td>\n",
              "      <td>1.303818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>1.071400</td>\n",
              "      <td>1.294407</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.934700</td>\n",
              "      <td>1.306669</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.837100</td>\n",
              "      <td>1.312257</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/raid/sourab/peft/src/peft/utils/save_and_load.py:132: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0c6fd6a7f58b44a38ee923b49092f99a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "adapter_model.safetensors:   0%|          | 0.00/871M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "18087eef8b0d4e3ebd72e32d485571af",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "events.out.tfevents.1704183170.hf-dgx-01.3021454.0:   0%|          | 0.00/8.68k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6e30694521aa40429b97fb641f94b2d7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Upload 3 LFS files:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b8e5ae82077b4b2e81250f478f488eb9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "training_args.bin:   0%|          | 0.00/4.73k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "trainer.train()\n",
        "trainer.save_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f1f9c19",
      "metadata": {
        "id": "3f1f9c19"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34be5374-26cc-47f4-bcd6-e48ff5199759",
      "metadata": {
        "id": "34be5374-26cc-47f4-bcd6-e48ff5199759"
      },
      "source": [
        "## Loading the trained model and getting the predictions of the trained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d606954d-4ef9-47b9-8eb3-03f7fabc362e",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "f5b8e1b99d9e489f9436f1d66db89f29",
            "5b05657c77d84039a0b0466c2b6b14e0",
            "477ee155a6934155a6cf4faf8f65c0c3",
            "962e5e3c4c764b01ab4ac6a5736fd931",
            "cd9e9fddc87a42d6b473a01bda5e7b2a",
            "e99e740bc93241bbb839bcde4dd3ef05",
            "8523a9f570f84a7d98b3c2b6b2929018"
          ]
        },
        "id": "d606954d-4ef9-47b9-8eb3-03f7fabc362e",
        "outputId": "016ea2af-3a38-4c1f-85c7-3bf761104994"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/raid/sourab/transformers/src/transformers/utils/hub.py:122: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f5b8e1b99d9e489f9436f1d66db89f29",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "adapter_config.json:   0%|          | 0.00/720 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5b05657c77d84039a0b0466c2b6b14e0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "477ee155a6934155a6cf4faf8f65c0c3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/2.27k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "962e5e3c4c764b01ab4ac6a5736fd931",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/968k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cd9e9fddc87a42d6b473a01bda5e7b2a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "added_tokens.json:   0%|          | 0.00/138 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e99e740bc93241bbb839bcde4dd3ef05",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/716 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8523a9f570f84a7d98b3c2b6b2929018",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "adapter_model.safetensors:   0%|          | 0.00/871M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from peft import PeftModel, PeftConfig\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "\n",
        "peft_model_id = \"Sanjaytfg/openhathi_instruct\"\n",
        "device = \"cuda\"\n",
        "config = PeftConfig.from_pretrained(peft_model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path)\n",
        "tokenizer = AutoTokenizer.from_pretrained(peft_model_id)\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "model = PeftModel.from_pretrained(model, peft_model_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f21c47b4-3c50-43ad-905e-1296e196d5a9",
      "metadata": {
        "id": "f21c47b4-3c50-43ad-905e-1296e196d5a9",
        "outputId": "aefeee0e-8e37-4581-8fad-14034e4a2e5b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<s><|im_start|>user \n",
            "bro, ye generative AI kya hai?<|im_end|> \n",
            "<|im_start|>assistant \n",
            "Generative AI ek aisa technology hai jo natural language processing aur machine learning ka use karke human-like content generation karne mein madad karta hai.<|im_end|>\n"
          ]
        }
      ],
      "source": [
        "# Diye gaye sentence ko English mein translate kare:\\n  Maine 10 baar customer care ko call kiya hai par abhi tak mujhe mera order nahi mila hai.\n",
        "# bro, ye generative AI kya hai?\n",
        "# भारत में पर्यटन के बारे में एक निबंध लिखें।\n",
        "\n",
        "model.to(torch.float16)\n",
        "model.cuda()\n",
        "model.eval()\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"bro, ye generative AI kya hai?\"},\n",
        "]\n",
        "text = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")#, add_special_tokens=False)\n",
        "inputs = {k: v.to(\"cuda\") for k,v in inputs.items()}\n",
        "outputs = model.generate(**inputs,\n",
        "                         max_new_tokens=128,\n",
        "                         do_sample=True,\n",
        "                         top_p=0.95,\n",
        "                         temperature=0.2,\n",
        "                         repetition_penalty=1.1,\n",
        "                         eos_token_id=tokenizer.eos_token_id)\n",
        "print(tokenizer.decode(outputs[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc7b18b2-f718-43b4-aad8-baa4e1d2dab7",
      "metadata": {
        "id": "cc7b18b2-f718-43b4-aad8-baa4e1d2dab7"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}